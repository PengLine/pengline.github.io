<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AI 模型的多种轻量化压缩技术 | 余一叶知秋尽</title><meta name="author" content="余一叶知秋尽"><meta name="copyright" content="余一叶知秋尽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="模型轻量化不是单一技术，而是组合策略（轻量结构设计 → 知识蒸馏 → 结构化剪枝 → 量化感知训练 → 硬件部署）。">
<meta property="og:type" content="article">
<meta property="og:title" content="AI 模型的多种轻量化压缩技术">
<meta property="og:url" content="https://pengline.github.io/2025/10/1332727d5b8a4fdc9cfbfd3661f009d4/index.html">
<meta property="og:site_name" content="余一叶知秋尽">
<meta property="og:description" content="模型轻量化不是单一技术，而是组合策略（轻量结构设计 → 知识蒸馏 → 结构化剪枝 → 量化感知训练 → 硬件部署）。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.toopic.cn/public/uploads/small/1755144020384175514402028.jpg">
<meta property="article:published_time" content="2025-10-14T16:00:00.000Z">
<meta property="article:modified_time" content="2025-10-15T02:55:19.834Z">
<meta property="article:author" content="余一叶知秋尽">
<meta property="article:tag" content="AI模型轻量化">
<meta property="article:tag" content="模型剪枝">
<meta property="article:tag" content="量化感知">
<meta property="article:tag" content="知识蒸馏">
<meta property="article:tag" content="低秩分解">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.toopic.cn/public/uploads/small/1755144020384175514402028.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "AI 模型的多种轻量化压缩技术",
  "url": "https://pengline.github.io/2025/10/1332727d5b8a4fdc9cfbfd3661f009d4/",
  "image": "https://www.toopic.cn/public/uploads/small/1755144020384175514402028.jpg",
  "datePublished": "2025-10-14T16:00:00.000Z",
  "dateModified": "2025-10-15T02:55:19.834Z",
  "author": [
    {
      "@type": "Person",
      "name": "余一叶知秋尽",
      "url": "https://pengline.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://pengline.github.io/2025/10/1332727d5b8a4fdc9cfbfd3661f009d4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":30,"languages":{"author":"作者: 余一叶知秋尽","link":"链接: ","source":"来源: 余一叶知秋尽","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AI 模型的多种轻量化压缩技术',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/font.css"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="余一叶知秋尽" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(/img/rqerwc4u0er.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">140</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list-ul"></i><span> 归档</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-outdent"></i><span> AI</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ai/aigc/"><i class="fa-fw fas fa-cubes"></i><span> AIGC创意平台</span></a></li><li><a class="site-page child" href="/ai/agent/medical/"><i class="fa-fw fas fa-medkit"></i><span> 医疗问答Agent</span></a></li><li><a class="site-page child" href="/ai/agent/stocks/"><i class="fa-fw fas fa-line-chart"></i><span> 股票分析Agent</span></a></li><li><a class="site-page child" href="/ai/agent/study/"><i class="fa-fw fas fa-book"></i><span> 教育学习Agent</span></a></li><li><a class="site-page child" href="/ai/agent/movie/"><i class="fa-fw fas fa-video"></i><span> 剧本创作Agent</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-cogs"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json/"><i class="fa-fw fas fa-cog"></i><span> JSON格式化</span></a></li><li><a class="site-page child" href="/tools/image/compressor/"><i class="fa-fw fas fa-image"></i><span> 图像压缩</span></a></li><li><a class="site-page child" href="/tools/image/watermark/"><i class="fa-fw fas fa-image"></i><span> 图像加水印</span></a></li><li><a class="site-page child" href="/tools/video/cropping/"><i class="fa-fw fas fa-cut"></i><span> 视频裁剪</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-pencil-square"></i><span> 文学</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/literature/poems"><i class="fa-fw fas fa-align-center"></i><span> 三行诗集</span></a></li><li><a class="site-page child" href="/literature/classical"><i class="fa-fw fas fa-align-justify"></i><span> 诗词歌赋</span></a></li><li><a class="site-page child" href="/literature/modern"><i class="fa-fw fas fa-align-left"></i><span> 现代诗歌</span></a></li><li><a class="site-page child" href="/literature/journal"><i class="fa-fw fas fa-align-right"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/literature/incomplete"><i class="fa-fw fas fa-columns"></i><span> 残句</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-commenting"></i><span> 交流</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://www.toopic.cn/public/uploads/small/1755144020384175514402028.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "/img/20241215205335173426721579192.jpg" data-lazy-src="/img/logo.png" alt="Logo"><span class="site-name">余一叶知秋尽</span></a><a class="nav-page-title" href="/"><span class="site-name">AI 模型的多种轻量化压缩技术</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list-ul"></i><span> 归档</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-outdent"></i><span> AI</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/ai/aigc/"><i class="fa-fw fas fa-cubes"></i><span> AIGC创意平台</span></a></li><li><a class="site-page child" href="/ai/agent/medical/"><i class="fa-fw fas fa-medkit"></i><span> 医疗问答Agent</span></a></li><li><a class="site-page child" href="/ai/agent/stocks/"><i class="fa-fw fas fa-line-chart"></i><span> 股票分析Agent</span></a></li><li><a class="site-page child" href="/ai/agent/study/"><i class="fa-fw fas fa-book"></i><span> 教育学习Agent</span></a></li><li><a class="site-page child" href="/ai/agent/movie/"><i class="fa-fw fas fa-video"></i><span> 剧本创作Agent</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-cogs"></i><span> 工具</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/json/"><i class="fa-fw fas fa-cog"></i><span> JSON格式化</span></a></li><li><a class="site-page child" href="/tools/image/compressor/"><i class="fa-fw fas fa-image"></i><span> 图像压缩</span></a></li><li><a class="site-page child" href="/tools/image/watermark/"><i class="fa-fw fas fa-image"></i><span> 图像加水印</span></a></li><li><a class="site-page child" href="/tools/video/cropping/"><i class="fa-fw fas fa-cut"></i><span> 视频裁剪</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-pencil-square"></i><span> 文学</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/literature/poems"><i class="fa-fw fas fa-align-center"></i><span> 三行诗集</span></a></li><li><a class="site-page child" href="/literature/classical"><i class="fa-fw fas fa-align-justify"></i><span> 诗词歌赋</span></a></li><li><a class="site-page child" href="/literature/modern"><i class="fa-fw fas fa-align-left"></i><span> 现代诗歌</span></a></li><li><a class="site-page child" href="/literature/journal"><i class="fa-fw fas fa-align-right"></i><span> 随笔</span></a></li><li><a class="site-page child" href="/literature/incomplete"><i class="fa-fw fas fa-columns"></i><span> 残句</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-commenting"></i><span> 交流</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">AI 模型的多种轻量化压缩技术</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-14T16:00:00.000Z" title="发表于 2025-10-15 00:00:00">2025-10-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-15T02:55:19.834Z" title="更新于 2025-10-15 10:55:19">2025-10-15</time></span></div><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Model/">Model</a></span><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>模型轻量化是深度学习部署中的关键技术，旨在在保持模型性能（如准确率）的前提下，显著减少模型的参数量、计算量（FLOPs）、内存占用和推理延迟，使其适用于资源受限的设备（如移动端、嵌入式设备、IoT设备等）。</p>
<p>通过系统化实施，可在移动端实现 &lt;10ms 延迟、&lt;5MB 模型体积，同时保持 90%+ 原始精度。</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><strong>精度-效率权衡</strong>：轻量化必然带来精度损失，需根据业务容忍度调整。</p>
</li>
<li class="lvl-2">
<p><strong>硬件适配</strong>：INT8在GPU/NPU上加速明显，但在CPU上可能收效甚微。</p>
</li>
<li class="lvl-2">
<p><strong>端到端延迟</strong>：不仅看FLOPs，还要考虑内存带宽、缓存命中率。</p>
</li>
<li class="lvl-2">
<p><strong>动态输入</strong>：部分轻量化方法（如通道剪枝）不支持动态shape。</p>
</li>
</ul>
</blockquote>
<p>（内容逐步完善中）</p>
<h2 id="技术手段与路线">技术手段与路线</h2>
<h3 id="网络结构设计">网络结构设计</h3>
<p>从源头设计轻量级网络结构。</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p>MobileNet（V1/V2/V3）：使用深度可分离卷积（Depthwise Separable Convolution）</p>
</li>
<li class="lvl-2">
<p>ShuffleNet（V1/V2）：使用通道混洗（Channel Shuffle）和分组卷积</p>
</li>
<li class="lvl-2">
<p>EfficientNet：通过复合缩放（Compound Scaling）平衡深度、宽度和分辨率</p>
</li>
<li class="lvl-2">
<p>GhostNet：通过廉价操作生成“幻影”特征图</p>
</li>
</ul>
</blockquote>
<p><strong>实施步骤</strong>：</p>
<ol>
<li class="lvl-3">
<p><strong>需求分析</strong>：明确目标设备（如手机、嵌入式芯片）、延迟/功耗/精度要求。</p>
</li>
<li class="lvl-3">
<p><strong>选择基线架构</strong>：根据任务（图像分类、目标检测等）选择合适的轻量网络（如MobileNetV2用于分类）。</p>
</li>
<li class="lvl-3">
<p><strong>调整超参数</strong>：调节宽度乘子（width multiplier）、分辨率等控制模型大小。</p>
</li>
<li class="lvl-3">
<p><strong>训练与验证</strong>：在目标任务数据集上训练，并验证精度与速度的平衡。</p>
</li>
</ol>
<h3 id="模型剪枝（Pruning）"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Pruning">模型剪枝</a>（Pruning）</h3>
<p><strong>模型剪枝</strong>是一种模型压缩技术，通过移除神经网络中不重要的参数（权重、神经元、通道等）来减少模型大小和计算量，同时尽量保持模型性能。</p>
<blockquote>
<p>移除冗余的权重、通道或层。</p>
<ul class="lvl-1">
<li class="lvl-2"><strong>权重剪枝</strong>（Unstructured Pruning）：移除绝对值小的权重，形成稀疏矩阵。需要稀疏计算支持（如专用硬件或库）。</li>
<li class="lvl-2"><strong>通道/结构化剪枝</strong>（Structured Pruning）：移除整个卷积核或通道，保持稠密结构，兼容通用硬件。</li>
</ul>
<p>原始模型 → 识别重要参数 → 移除不重要参数 → 微调 → 剪枝后模型</p>
</blockquote>
<p>剪枝的粒度级别</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p>“权重级”: “移除单个权重参数”,</p>
</li>
<li class="lvl-2">
<p>“神经元级”: “移除整个神经元”,</p>
</li>
<li class="lvl-2">
<p>“通道级”: “移除整个特征通道”,</p>
</li>
<li class="lvl-2">
<p>“层级”: “移除整个网络层”,</p>
</li>
<li class="lvl-2">
<p>“块级”: “移除网络块（如ResNet块）”</p>
</li>
</ul>
</blockquote>
<h4 id="剪枝技术和类别">剪枝技术和类别</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p>结构剪枝</p>
<ul class="lvl-2">
<li class="lvl-4">
<p>非结构化剪枝 - 移除单个权重</p>
</li>
<li class="lvl-4">
<p>结构化剪枝 - 移除整个通道/神经元</p>
</li>
</ul>
</li>
<li class="lvl-2">
<p>剪枝技术</p>
<ul class="lvl-2">
<li class="lvl-4"><strong>迭代剪枝</strong> - 逐步剪枝并微调</li>
<li class="lvl-4">基于<strong>正则化</strong>的剪枝
<ul class="lvl-4">
<li class="lvl-6">L1正则化损失 - 促进稀疏性</li>
<li class="lvl-6">组Lasso损失 - 促进结构化稀疏性</li>
<li class="lvl-6">组合损失函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="核心算法">核心算法</h4>
<p>基于<strong>幅度</strong> 和 <strong>梯度</strong> 算法的剪枝</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>基于幅度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MagnitudeBasedPruning</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于权重大小的剪枝方法&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pruning_rate=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.pruning_rate = pruning_rate</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">global_magnitude_prune</span>(<span class="params">self, model</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;全局幅度剪枝&quot;&quot;&quot;</span></span><br><span class="line">        all_weights = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 收集所有权重</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name <span class="keyword">and</span> <span class="built_in">len</span>(param.shape) &gt; <span class="number">1</span>:  <span class="comment"># 只处理权重矩阵</span></span><br><span class="line">                all_weights.append(param.data.<span class="built_in">abs</span>().view(-<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        all_weights = torch.cat(all_weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算全局阈值</span></span><br><span class="line">        threshold = torch.quantile(all_weights, <span class="variable language_">self</span>.pruning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用剪枝</span></span><br><span class="line">        masks = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name <span class="keyword">and</span> <span class="built_in">len</span>(param.shape) &gt; <span class="number">1</span>:</span><br><span class="line">                mask = param.data.<span class="built_in">abs</span>() &gt; threshold</span><br><span class="line">                masks[name] = mask</span><br><span class="line">                param.data *= mask.<span class="built_in">float</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> masks</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">layer_wise_magnitude_prune</span>(<span class="params">self, model</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;逐层幅度剪枝&quot;&quot;&quot;</span></span><br><span class="line">        masks = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, (nn.Linear, nn.Conv2d)):</span><br><span class="line">                <span class="comment"># 计算该层的阈值</span></span><br><span class="line">                weights = module.weight.data.<span class="built_in">abs</span>().view(-<span class="number">1</span>)</span><br><span class="line">                threshold = torch.quantile(weights, <span class="variable language_">self</span>.pruning_rate)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 创建掩码</span></span><br><span class="line">                mask = module.weight.data.<span class="built_in">abs</span>() &gt; threshold</span><br><span class="line">                masks[name] = mask</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 应用剪枝</span></span><br><span class="line">                module.weight.data *= mask.<span class="built_in">float</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> masks</span><br></pre></td></tr></table></figure>
</li>
<li class="lvl-2">
<p>基于梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GradientBasedPruning</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于梯度信息的剪枝方法&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pruning_rate=<span class="number">0.2</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.pruning_rate = pruning_rate</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_weight_importance</span>(<span class="params">self, model, dataloader, criterion</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算权重重要性（基于梯度）&quot;&quot;&quot;</span></span><br><span class="line">        model.train()</span><br><span class="line">        importance_scores = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化重要性分数</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">                importance_scores[name] = torch.zeros_like(param.data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算梯度并累积重要性</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            model.zero_grad()</span><br><span class="line">            output = model(data)</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            loss.backward()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 累积梯度信息</span></span><br><span class="line">            <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name <span class="keyword">and</span> param.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="comment"># 使用梯度×权重作为重要性指标</span></span><br><span class="line">                    importance_scores[name] += (param.data * param.grad).<span class="built_in">abs</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> importance_scores</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">gradient_based_prune</span>(<span class="params">self, model, dataloader, criterion</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;基于梯度的剪枝&quot;&quot;&quot;</span></span><br><span class="line">        importance_scores = <span class="variable language_">self</span>.compute_weight_importance(model, dataloader, criterion)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算全局阈值</span></span><br><span class="line">        all_scores = torch.cat([score.view(-<span class="number">1</span>) <span class="keyword">for</span> score <span class="keyword">in</span> importance_scores.values()])</span><br><span class="line">        threshold = torch.quantile(all_scores, <span class="variable language_">self</span>.pruning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用剪枝</span></span><br><span class="line">        masks = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">                mask = importance_scores[name] &gt; threshold</span><br><span class="line">                masks[name] = mask</span><br><span class="line">                param.data *= mask.<span class="built_in">float</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> masks</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="实施步骤">实施步骤</h4>
<ol>
<li class="lvl-3">
<p><strong>预训练模型</strong>：在目标任务上训练一个完整模型。</p>
</li>
<li class="lvl-3">
<p>重要性评估：</p>
<ul class="lvl-2">
<li class="lvl-5">L1/L2范数：通道权重的L1范数越小越不重要。</li>
<li class="lvl-5">梯度信息：使用泰勒展开估计移除某通道对损失的影响。</li>
<li class="lvl-5">注意力机制：如使用BN层的γ系数作为通道重要性指标。</li>
</ul>
</li>
<li class="lvl-3">
<p>剪枝策略：</p>
<ul class="lvl-2">
<li class="lvl-5">一次性剪枝（One-shot）：直接剪掉一定比例。</li>
<li class="lvl-5">迭代剪枝（Iterative）：逐步剪枝 + 微调，效果更好。</li>
</ul>
</li>
<li class="lvl-3">
<p><strong>微调（Fine-tuning）</strong>：恢复因剪枝导致的精度下降。</p>
</li>
<li class="lvl-3">
<p><strong>导出剪枝后模型</strong>：移除冗余参数，生成紧凑模型。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AutoPruner</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自动剪枝框架&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, dataloader, criterion</span>):</span><br><span class="line">        <span class="variable language_">self</span>.model = model</span><br><span class="line">        <span class="variable language_">self</span>.dataloader = dataloader</span><br><span class="line">        <span class="variable language_">self</span>.criterion = criterion</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sensitivity_analysis</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;敏感性分析 - 确定各层对剪枝的敏感度&quot;&quot;&quot;</span></span><br><span class="line">        sensitivity_scores = &#123;&#125;</span><br><span class="line">        original_accuracy = <span class="variable language_">self</span>.evaluate_model()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> <span class="variable language_">self</span>.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, (nn.Linear, nn.Conv2d)):</span><br><span class="line">                <span class="comment"># 测试不同剪枝比例的影响</span></span><br><span class="line">                sensitivities = []</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> amount <span class="keyword">in</span> [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.7</span>]:</span><br><span class="line">                    <span class="comment"># 创建临时副本</span></span><br><span class="line">                    temp_model = copy.deepcopy(<span class="variable language_">self</span>.model)</span><br><span class="line">                    temp_module = <span class="built_in">dict</span>(temp_model.named_modules())[name]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 应用剪枝</span></span><br><span class="line">                    prune.l1_unstructured(temp_module, name=<span class="string">&#x27;weight&#x27;</span>, amount=amount)</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 评估性能</span></span><br><span class="line">                    accuracy = <span class="variable language_">self</span>.evaluate_model(temp_model)</span><br><span class="line">                    accuracy_drop = original_accuracy - accuracy</span><br><span class="line">                    sensitivities.append(accuracy_drop)</span><br><span class="line">                </span><br><span class="line">                sensitivity_scores[name] = &#123;</span><br><span class="line">                    <span class="string">&#x27;sensitivities&#x27;</span>: sensitivities,</span><br><span class="line">                    <span class="string">&#x27;avg_drop&#x27;</span>: <span class="built_in">sum</span>(sensitivities) / <span class="built_in">len</span>(sensitivities)</span><br><span class="line">                &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> sensitivity_scores</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate_model</span>(<span class="params">self, model=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;评估模型性能&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> model <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            model = <span class="variable language_">self</span>.model</span><br><span class="line">        </span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> data, target <span class="keyword">in</span> <span class="variable language_">self</span>.dataloader:</span><br><span class="line">                output = model(data)</span><br><span class="line">                pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">                correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line">                total += target.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> correct / total</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">auto_prune</span>(<span class="params">self, target_sparsity=<span class="number">0.7</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;自动剪枝&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 敏感性分析</span></span><br><span class="line">        sensitivity_scores = <span class="variable language_">self</span>.sensitivity_analysis()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 根据敏感度分配剪枝比例</span></span><br><span class="line">        pruning_plan = <span class="variable language_">self</span>.create_pruning_plan(sensitivity_scores, target_sparsity)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用剪枝</span></span><br><span class="line">        pruner = ComprehensivePruner(<span class="variable language_">self</span>.model)</span><br><span class="line">        pruner.apply_pruning(pruning_plan)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> pruning_plan</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_pruning_plan</span>(<span class="params">self, sensitivity_scores, target_sparsity</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;创建剪枝计划&quot;&quot;&quot;</span></span><br><span class="line">        pruning_plan = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 按敏感度排序（敏感度低的层可以剪枝更多）</span></span><br><span class="line">        sorted_layers = <span class="built_in">sorted</span>(sensitivity_scores.items(), </span><br><span class="line">                             key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>][<span class="string">&#x27;avg_drop&#x27;</span>])</span><br><span class="line">        </span><br><span class="line">        total_layers = <span class="built_in">len</span>(sorted_layers)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, (layer_name, scores) <span class="keyword">in</span> <span class="built_in">enumerate</span>(sorted_layers):</span><br><span class="line">            <span class="comment"># 敏感度低的层分配更高的剪枝比例</span></span><br><span class="line">            base_amount = target_sparsity * (<span class="number">1</span> - i / total_layers)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 根据具体敏感度调整</span></span><br><span class="line">            sensitivity_factor = <span class="number">1</span> - <span class="built_in">min</span>(scores[<span class="string">&#x27;avg_drop&#x27;</span>] * <span class="number">10</span>, <span class="number">0.8</span>)</span><br><span class="line">            final_amount = base_amount * sensitivity_factor</span><br><span class="line">            </span><br><span class="line">            pruning_plan[layer_name] = &#123;</span><br><span class="line">                <span class="string">&#x27;method&#x27;</span>: <span class="string">&#x27;l1_unstructured&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;amount&#x27;</span>: final_amount</span><br><span class="line">            &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> pruning_plan</span><br></pre></td></tr></table></figure>
<h3 id="量化感知（Quantization）"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Quantization">量化感知</a>（Quantization）</h3>
<p>将浮点数（FP32）转换为低比特整数（INT8/INT4）或浮点（FP16）。需校准确定量化范围（min/max或scale/zero-point）</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><strong>训练后量化（PTQ, Post-Training Quantization）</strong>：无需重新训练，速度快。</p>
</li>
<li class="lvl-2">
<p><strong>量化感知训练（QAT, Quantization-Aware Training）</strong>：在训练中模拟量化误差，精度更高。</p>
</li>
</ul>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">指标</th>
<th style="text-align:left">FP32模型</th>
<th style="text-align:left">INT8量化后</th>
<th style="text-align:left">INT4量化后</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">模型大小（7B参数）</td>
<td style="text-align:left">~28GB</td>
<td style="text-align:left">~7GB</td>
<td style="text-align:left">~3.5GB</td>
</tr>
<tr>
<td style="text-align:left">显存占用（推理）</td>
<td style="text-align:left">14GB</td>
<td style="text-align:left">7GB</td>
<td style="text-align:left">3.5GB</td>
</tr>
<tr>
<td style="text-align:left">推理速度</td>
<td style="text-align:left">基准1x</td>
<td style="text-align:left">1.5-2x更快</td>
<td style="text-align:left">2-3x更快</td>
</tr>
<tr>
<td style="text-align:left">精度损失</td>
<td style="text-align:left">无</td>
<td style="text-align:left">&lt;1% (通常可接受)</td>
<td style="text-align:left">1-3% (可能需调优)</td>
</tr>
</tbody>
</table>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><strong>原始精度</strong>：模型训练时通常使用FP32（32位浮点数），每个参数占用4字节。</p>
</li>
<li class="lvl-2">
<p><strong>量化后</strong>：将参数转换为低精度格式（如INT8、INT4），每个参数仅占1字节甚至0.5字节。</p>
<ul class="lvl-3">
<li class="lvl-4">例如：将权重从 <code>0.8732</code>（FP32）近似为 <code>0.875</code>（INT8）。</li>
</ul>
</li>
</ul>
</blockquote>
<p><strong>量化的主要类型</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">类型</th>
<th style="text-align:left">说明</th>
<th style="text-align:left">典型应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>训练后量化</strong></td>
<td style="text-align:left">对已训练好的模型直接量化（无需重新训练）</td>
<td style="text-align:left">快速部署，资源有限环境</td>
</tr>
<tr>
<td style="text-align:left"><strong>量化感知训练</strong></td>
<td style="text-align:left">在训练过程中模拟量化误差，使模型适应低精度</td>
<td style="text-align:left">高精度要求的微调场景</td>
</tr>
<tr>
<td style="text-align:left"><strong>动态量化</strong></td>
<td style="text-align:left">推理时动态量化权重和激活值</td>
<td style="text-align:left">实时性要求高的场景</td>
</tr>
<tr>
<td style="text-align:left"><strong>静态量化</strong></td>
<td style="text-align:left">预先校准量化参数（如缩放因子），推理时固定</td>
<td style="text-align:left">移动端/嵌入式设备</td>
</tr>
</tbody>
</table>
<p><strong>量化的实现技术</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">方法路径</th>
<th style="text-align:left">核心思路</th>
<th style="text-align:left">适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>GPTQ量化</strong></td>
<td style="text-align:left">一种训练后量化方法，尤其适合降低大语言模型的显存占用和提升推理速度。</td>
<td style="text-align:left">希望获得极致性能，对精度损失有一定容忍度。</td>
</tr>
<tr>
<td style="text-align:left"><strong>PyTorch原生量化</strong></td>
<td style="text-align:left">使用<code>torch.ao.quantization</code>等PyTorch内置模块，支持动态和静态量化。</td>
<td style="text-align:left">需要官方支持，与PyTorch生态紧密结合。</td>
</tr>
<tr>
<td style="text-align:left"><strong>TensorRT集成</strong></td>
<td style="text-align:left">通过转换模型至ONNX格式，再利用TensorRT进行优化和量化，能显著提升推理速度。</td>
<td style="text-align:left">生产环境部署，追求NVIDIA硬件上的最高推理性能。</td>
</tr>
</tbody>
</table>
<h4 id="GPTQ-量化">GPTQ 量化</h4>
<h4 id="PyTorch-量化">PyTorch 量化</h4>
<h4 id="TensorRT-集成">TensorRT 集成</h4>
<h4 id="具体实施">具体实施</h4>
<p><strong>实施步骤（以QAT为例）</strong>：</p>
<ol>
<li class="lvl-3">
<p><strong>插入伪量化节点</strong>：在训练图中插入FakeQuant操作，模拟量化过程。</p>
</li>
<li class="lvl-3">
<p><strong>使用量化感知损失函数</strong>：保持梯度可传。</p>
</li>
<li class="lvl-3">
<p><strong>训练/微调模型</strong>：通常只需少量epoch。</p>
</li>
<li class="lvl-3">
<p><strong>转换为实际量化模型</strong>：将FakeQuant替换为真实INT8操作。</p>
</li>
<li class="lvl-3">
<p><strong>部署</strong>：使用支持INT8的推理引擎（如TensorRT、ONNX Runtime、TFLite）。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> Quantization.data <span class="keyword">import</span> get_sample_input</span><br><span class="line"><span class="keyword">from</span> torch.ao.quantization <span class="keyword">import</span> (</span><br><span class="line">    get_default_qconfig_mapping,</span><br><span class="line">    prepare,</span><br><span class="line">    convert,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quantize_model_fp32_to_int8</span>(<span class="params">model_fp32: nn.Module, calib_data</span>) -&gt; torch.nn.Module:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用训练后量化（PTQ）将 FP32 模型转为 INT8（仅 CPU 支持）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model_fp32: 原始 FP32 模型（必须是 eval 模式）</span></span><br><span class="line"><span class="string">        calib_data: 校准数据，用于确定量化范围（min/max）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        量化后的 INT8 模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Step 1: 设置量化配置（仅支持 CPU 后端）</span></span><br><span class="line">    qconfig_mapping = get_default_qconfig_mapping(<span class="string">&quot;x86&quot;</span>)  <span class="comment"># 或 &quot;fbgemm&quot;（Linux）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: 准备模型（插入 observer）</span></span><br><span class="line">    example_inputs = (calib_data[<span class="number">0</span>], calib_data[<span class="number">1</span>])</span><br><span class="line">    model_prepared = prepare(model_fp32, qconfig_mapping, example_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: 校准（用少量数据统计激活值范围）</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;正在校准模型...&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 10 个 batch 足够</span></span><br><span class="line">            x, x_lens = get_sample_input()</span><br><span class="line">            model_prepared(x, x_lens)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: 转换为量化模型</span></span><br><span class="line">    model_int8 = convert(model_prepared)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;量化完成！&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> model_int8</span><br></pre></td></tr></table></figure>
<blockquote>
<p>为什么量化后模型大小没变？</p>
<p>原因：<code>state_dict()</code> 保存的是 <strong>量化参数（scale/zero_point）+ 量化权重（int8）</strong>，但 <strong>PyTorch 默认以 float32 格式序列化所有张量</strong>！即使权重是 <code>int8</code>，当你调用 <code>torch.save(model.state_dict())</code> 时：</p>
<ul class="lvl-1">
<li class="lvl-2">
<p>PyTorch 会把 <code>int8</code> 张量<strong>自动转换为 <code>float32</code></strong> 存储（为了兼容性）；</p>
</li>
<li class="lvl-2">
<p>同时还保存了 <code>scale</code>、<code>zero_point</code> 等额外参数；</p>
</li>
</ul>
<p><strong>最终文件大小 ≈ 原始 FP32 模型</strong>，甚至更大！</p>
<p><strong>量化的核心收益在运行时，不在存储文件</strong>。部署时应使用 <strong>TorchScript / ONNX</strong> 格式。</p>
</blockquote>
<h3 id="知识蒸馏（Distillation）"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Knowledge">知识蒸馏</a>（Distillation）</h3>
<p><strong>知识蒸馏</strong>（Knowledge Distillation）是一种模型压缩技术，其核心思想是将一个庞大、复杂但性能优异的模型（教师模型）的知识转移到一个更小、更高效的模型（学生模型）中。</p>
<blockquote>
<p>用大模型（Teacher）指导小模型（Student）学习。</p>
<p>教师模型 (大而复杂) → 知识转移 → 学生模型 (小而高效)。</p>
<p><strong>知识蒸馏的成功， 80% 取决于训练数据质量</strong> ，且<strong>Teacher 和 Student 差距不宜过大</strong></p>
</blockquote>
<p><strong>优势</strong>：</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><strong>模型压缩</strong>：大幅减少参数量和计算量</p>
</li>
<li class="lvl-2">
<p><strong>性能保持</strong>：学生模型性能接近教师模型</p>
</li>
<li class="lvl-2">
<p><strong>推理加速</strong>：更快的推理速度</p>
</li>
<li class="lvl-2">
<p><strong>部署友好</strong>：适合资源受限环境</p>
</li>
</ul>
</blockquote>
<p><strong>挑战</strong>：</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><strong>教师模型质量依赖</strong>：教师模型质量直接影响蒸馏效果</p>
</li>
<li class="lvl-2">
<p><strong>超参数敏感</strong>：温度、权重系数等需要仔细调优</p>
</li>
<li class="lvl-2">
<p><strong>训练复杂度</strong>：需要同时训练教师和学生模型</p>
</li>
<li class="lvl-2">
<p><strong>知识损失</strong>：不可避免会损失部分知识</p>
</li>
</ul>
</blockquote>
<h4 id="核心原理">核心原理</h4>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>软标签</strong>：教师模型输出的概率分布包含更多信息</p>
</li>
<li class="lvl-2">
<p><strong>暗知识</strong>：类别之间的关系等隐含知识</p>
</li>
<li class="lvl-2">
<p><strong>温度参数</strong>：控制概率分布的平滑程度</p>
</li>
</ul>
<p>软标签与硬标签</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 硬标签 vs 软标签示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demonstrate_labels</span>():</span><br><span class="line">    <span class="comment"># 硬标签: [0, 0, 1, 0, 0]</span></span><br><span class="line">    hard_labels = torch.tensor([<span class="number">2</span>])  <span class="comment"># 只是类别索引</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 软标签: [0.1, 0.2, 0.5, 0.15, 0.05]</span></span><br><span class="line">    soft_labels = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.15</span>, <span class="number">0.05</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> hard_labels, soft_labels</span><br></pre></td></tr></table></figure>
<p>温度缩放：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KnowledgeDistillationLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, temperature=<span class="number">3.0</span>, alpha=<span class="number">0.7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.temperature = temperature</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="variable language_">self</span>.kl_loss = nn.KLDivLoss(reduction=<span class="string">&quot;batchmean&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.ce_loss = nn.CrossEntropyLoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, student_logits, teacher_logits, labels</span>):</span><br><span class="line">        <span class="comment"># 应用温度缩放</span></span><br><span class="line">        student_soft = F.log_softmax(student_logits / <span class="variable language_">self</span>.temperature, dim=<span class="number">1</span>)</span><br><span class="line">        teacher_soft = F.softmax(teacher_logits / <span class="variable language_">self</span>.temperature, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 蒸馏损失</span></span><br><span class="line">        distillation_loss = <span class="variable language_">self</span>.kl_loss(student_soft, teacher_soft) * (<span class="variable language_">self</span>.temperature ** <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学生损失</span></span><br><span class="line">        student_loss = <span class="variable language_">self</span>.ce_loss(student_logits, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 总损失</span></span><br><span class="line">        total_loss = <span class="variable language_">self</span>.alpha * distillation_loss + (<span class="number">1</span> - <span class="variable language_">self</span>.alpha) * student_loss</span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<h4 id="训练策略">训练策略</h4>
<p>渐进式蒸馏</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ProgressiveDistillation</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;渐进式知识蒸馏&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, total_epochs</span>):</span><br><span class="line">        <span class="variable language_">self</span>.total_epochs = total_epochs</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_distillation_weights</span>(<span class="params">self, current_epoch</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;动态调整蒸馏权重&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 早期更依赖教师，后期更依赖真实标签</span></span><br><span class="line">        alpha = <span class="built_in">max</span>(<span class="number">0.1</span>, <span class="number">0.7</span> * (<span class="number">1</span> - current_epoch / <span class="variable language_">self</span>.total_epochs))</span><br><span class="line">        temperature = <span class="built_in">max</span>(<span class="number">1.0</span>, <span class="number">3.0</span> * (<span class="number">1</span> - current_epoch / <span class="variable language_">self</span>.total_epochs))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> alpha, temperature</span><br></pre></td></tr></table></figure>
<p>注意力转移</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDistillation</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;注意力机制蒸馏&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.mse_loss = nn.MSELoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_attention_loss</span>(<span class="params">self, student_attentions, teacher_attentions</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算注意力蒸馏损失&quot;&quot;&quot;</span></span><br><span class="line">        attention_loss = <span class="number">0</span></span><br><span class="line">        num_layers = <span class="built_in">min</span>(<span class="built_in">len</span>(student_attentions), <span class="built_in">len</span>(teacher_attentions))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            s_attn = student_attentions[i]  <span class="comment"># [batch, heads, seq_len, seq_len]</span></span><br><span class="line">            t_attn = teacher_attentions[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 确保头数匹配</span></span><br><span class="line">            <span class="keyword">if</span> s_attn.shape[<span class="number">1</span>] != t_attn.shape[<span class="number">1</span>]:</span><br><span class="line">                <span class="comment"># 平均池化调整头数</span></span><br><span class="line">                <span class="keyword">if</span> s_attn.shape[<span class="number">1</span>] &lt; t_attn.shape[<span class="number">1</span>]:</span><br><span class="line">                    <span class="comment"># 学生头数少，对教师注意力求平均</span></span><br><span class="line">                    t_attn = t_attn.mean(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 学生头数多，复制教师注意力</span></span><br><span class="line">                    t_attn = t_attn.repeat(<span class="number">1</span>, s_attn.shape[<span class="number">1</span>] // t_attn.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            layer_loss = <span class="variable language_">self</span>.mse_loss(s_attn, t_attn)</span><br><span class="line">            attention_loss += layer_loss</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> attention_loss / num_layers</span><br></pre></td></tr></table></figure>
<h4 id="实施步骤-2">实施步骤</h4>
<ol>
<li class="lvl-3">
<p><strong>训练Teacher模型</strong>：高精度但复杂的大模型。</p>
</li>
<li class="lvl-3">
<p><strong>设计Student模型</strong>：结构更轻量（如层数更少、通道更窄）。</p>
</li>
<li class="lvl-3">
<p>定义蒸馏损失：</p>
<ul class="lvl-2">
<li class="lvl-5"><strong>软标签损失</strong>（Soft Target）：使用Teacher输出的softmax logits（温度缩放）。</li>
<li class="lvl-5"><strong>特征图对齐</strong>：中间层特征的L2或注意力对齐。</li>
</ul>
</li>
<li class="lvl-3">
<p><strong>联合训练</strong>：Student同时学习真实标签和Teacher的输出。</p>
</li>
<li class="lvl-3">
<p><strong>部署Student模型</strong>：独立使用，无需Teacher。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2Config, GPT2LMHeadModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TeacherStudentDistillation</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;完整的知识蒸馏实现&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, teacher_model, student_model, temperature=<span class="number">3.0</span>, alpha=<span class="number">0.7</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.teacher_model = teacher_model</span><br><span class="line">        <span class="variable language_">self</span>.student_model = student_model</span><br><span class="line">        <span class="variable language_">self</span>.temperature = temperature</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 冻结教师模型参数</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.teacher_model.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 损失函数</span></span><br><span class="line">        <span class="variable language_">self</span>.kl_loss = nn.KLDivLoss(reduction=<span class="string">&quot;batchmean&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.ce_loss = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="variable language_">self</span>.mse_loss = nn.MSELoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_models</span>():</span><br><span class="line">        <span class="string">&quot;&quot;&quot;创建教师和学生模型&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 教师模型 (大模型)</span></span><br><span class="line">        teacher_config = GPT2Config(</span><br><span class="line">            n_layer=<span class="number">12</span>,  <span class="comment"># 12层</span></span><br><span class="line">            n_head=<span class="number">12</span>,   <span class="comment"># 12个注意力头</span></span><br><span class="line">            n_embd=<span class="number">768</span>   <span class="comment"># 768维嵌入</span></span><br><span class="line">        )</span><br><span class="line">        teacher_model = GPT2LMHeadModel(teacher_config)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学生模型 (小模型)</span></span><br><span class="line">        student_config = GPT2Config(</span><br><span class="line">            n_layer=<span class="number">6</span>,   <span class="comment"># 6层</span></span><br><span class="line">            n_head=<span class="number">6</span>,    <span class="comment"># 6个注意力头  </span></span><br><span class="line">            n_embd=<span class="number">384</span>   <span class="comment"># 384维嵌入</span></span><br><span class="line">        )</span><br><span class="line">        student_model = GPT2LMHeadModel(student_config)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> teacher_model, student_model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_distillation_loss</span>(<span class="params">self, student_logits, teacher_logits, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;计算蒸馏损失&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 响应式知识蒸馏</span></span><br><span class="line">        student_soft = F.log_softmax(student_logits / <span class="variable language_">self</span>.temperature, dim=-<span class="number">1</span>)</span><br><span class="line">        teacher_soft = F.softmax(teacher_logits / <span class="variable language_">self</span>.temperature, dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        distillation_loss = <span class="variable language_">self</span>.kl_loss(student_soft, teacher_soft) * (<span class="variable language_">self</span>.temperature ** <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学生任务损失</span></span><br><span class="line">        task_loss = <span class="variable language_">self</span>.ce_loss(student_logits.view(-<span class="number">1</span>, student_logits.size(-<span class="number">1</span>)), </span><br><span class="line">                               labels.view(-<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 组合损失</span></span><br><span class="line">        total_loss = <span class="variable language_">self</span>.alpha * distillation_loss + (<span class="number">1</span> - <span class="variable language_">self</span>.alpha) * task_loss</span><br><span class="line">        <span class="keyword">return</span> total_loss</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, input_ids, attention_mask, labels</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;训练步骤&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 教师模型前向传播</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            teacher_outputs = <span class="variable language_">self</span>.teacher_model(</span><br><span class="line">                input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask,</span><br><span class="line">                labels=labels</span><br><span class="line">            )</span><br><span class="line">            teacher_logits = teacher_outputs.logits</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 学生模型前向传播</span></span><br><span class="line">        student_outputs = <span class="variable language_">self</span>.student_model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            labels=labels</span><br><span class="line">        )</span><br><span class="line">        student_logits = student_outputs.logits</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算蒸馏损失</span></span><br><span class="line">        loss = <span class="variable language_">self</span>.compute_distillation_loss(student_logits, teacher_logits, labels)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h4 id="Token-循环问题">Token 循环问题</h4>
<p>这是<strong>知识蒸馏失败的典型症状</strong>，模型陷入<strong>重复 token 循环</strong>。</p>
<blockquote>
<p>Student (原始): The future of AI is the real question.<br>
One of the first things I learned about AI is that it’s almost impossible to write algorithms in this way. It’s also a bit difficult to understand how any algorithm can perform any task. For example, in one of</p>
<p>Student (蒸馏后): The future of AI is and and and and and and and（大量重复token，也可能是其他符号或字符）</p>
</blockquote>
<p>出现此种问题的可能原因为：</p>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>训练数据不足/质量差</strong></p>
<p>模型无法学习通用语言模式，只能记住训练数据中的模式。如果训练数据中 <code>,</code> 出现频率高，模型会过度生成。</p>
<p>尽量使用<strong>真实训练数据</strong></p>
</li>
<li class="lvl-2">
<p><strong>蒸馏参数不当</strong></p>
<ul class="lvl-2">
<li class="lvl-4"><strong><code>alpha=0.7</code> 过高</strong>：Student 过度依赖 Teacher 的软标签。可以适当增加硬标签权重（0.3~0.5）</li>
<li class="lvl-4"><strong><code>temperature=2.0</code> 过高</strong>：Teacher 分布过于平滑，失去区分度。可以减少平滑（1.0~2.0）</li>
</ul>
</li>
<li class="lvl-2">
<p><strong>无真实标签监督</strong></p>
<p>硬标签损失（CE）权重仅 30%，模型未充分学习基本语言建模能力。</p>
<p>可以适当增加硬标签损失权重</p>
</li>
<li class="lvl-2">
<p><strong>Teacher 和 Student 差距过大</strong></p>
<p><code>gpt2-medium</code> (355M) → <code>gpt2</code> (124M)，小模型难以模仿大模型的复杂行为。</p>
<p>可以使用更小的 Teacher</p>
</li>
</ul>
<p>同时可添加惩罚重复属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">outputs = model.generate(</span><br><span class="line">            **inputs,</span><br><span class="line">            max_new_tokens=max_length,</span><br><span class="line">            repetition_penalty=<span class="number">1.2</span>,  <span class="comment"># 惩罚重复 token</span></span><br><span class="line">            do_sample=<span class="literal">True</span>,</span><br><span class="line">            temperature=<span class="number">0.7</span>,</span><br><span class="line">            pad_token_id=tokenizer.eos_token_id</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="低秩分解（Low-Rank）"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Low-Rank">低秩分解</a>（Low-Rank）</h3>
<p>将一个大权重矩阵/张量近似分解为多个小矩阵/张量的乘积，从而<strong>减少参数量和计算量</strong>。</p>
<ul class="lvl-0">
<li class="lvl-2">
<p>将卷积核 分解为两个更小的卷积。<br>
$$<br>
W∈R<br>
C<br>
out​×C<br>
in​×k×k<br>
$$</p>
</li>
<li class="lvl-2">
<p>使用SVD分解全连接层。</p>
</li>
</ul>
<blockquote>
<p>想象一个复杂的变换需要1000个输入和1000个输出，那么它的权重矩阵 <code>W</code> 的大小是 1000×1000，共有100万个参数。</p>
<p>低秩分解发现，这个变换的内在“自由度”或“信息量”其实没那么高（即它是<strong>低秩</strong>的）。</p>
<ol>
<li class="lvl-3">
<p>先将1000维输入<strong>投影</strong>到一个低维空间（比如50维）。这对应一个矩阵 <code>A</code> (1000×50)。</p>
</li>
<li class="lvl-3">
<p>再从这个50维空间<strong>恢复</strong>到1000维输出。这对应一个矩阵 <code>B</code> (50×1000)。</p>
</li>
</ol>
<p>于是：<code>W ≈ B × A</code></p>
<p><strong>参数总量从：</strong> 1000 × 1000 = 1,000,000<br>
<strong>减少到：</strong> 1000 × 50 + 50 × 1000 = 100,000<br>
<strong>压缩率高达90%。</strong></p>
</blockquote>
<p><strong>实施步骤</strong>：</p>
<ol>
<li class="lvl-3">
<p>对预训练模型的权重进行SVD或CP/Tucker分解。</p>
</li>
<li class="lvl-3">
<p>替换原层为分解后的多层结构。</p>
</li>
<li class="lvl-3">
<p>微调模型恢复精度。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decompose_linear_svd</span>(<span class="params">linear_layer: nn.Linear, rank_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用 SVD 对 Linear 层进行低秩分解</span></span><br><span class="line"><span class="string">    输入: Linear(in_features=n, out_features=m)</span></span><br><span class="line"><span class="string">    输出: (Linear(n, r), Linear(r, m))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(linear_layer, nn.Linear):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">f&quot;Expected nn.Linear, got <span class="subst">&#123;<span class="built_in">type</span>(linear_layer)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    W = linear_layer.weight.data  <span class="comment"># [m, n] = [256, 2560]</span></span><br><span class="line">    m, n = W.shape</span><br><span class="line"></span><br><span class="line">    max_rank = <span class="built_in">min</span>(m, n)  <span class="comment"># = 256</span></span><br><span class="line">    r = <span class="built_in">int</span>(rank_ratio * max_rank)</span><br><span class="line">    r = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">min</span>(r, max_rank - <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    U, S, Vh = torch.linalg.svd(W, full_matrices=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># print(f&quot;  U shape: &#123;U.shape&#125;&quot;)  # [256, 256]</span></span><br><span class="line">    <span class="comment"># print(f&quot;  S shape: &#123;S.shape&#125;&quot;)  # [256]</span></span><br><span class="line">    <span class="comment"># print(f&quot;  Vh shape: &#123;Vh.shape&#125;&quot;)  # [256, 2560] ← 必须是这个！</span></span><br><span class="line"></span><br><span class="line">    U_r = U[:, :r]  <span class="comment"># [256, r]</span></span><br><span class="line">    S_r = S[:r]  <span class="comment"># [r]</span></span><br><span class="line">    Vh_r = Vh[:r, :]  <span class="comment"># [r, 2560] ← 关键！</span></span><br><span class="line"></span><br><span class="line">    sqrt_S = torch.sqrt(S_r)</span><br><span class="line">    B = sqrt_S.unsqueeze(<span class="number">1</span>) * Vh_r  <span class="comment"># [r, 2560]</span></span><br><span class="line">    A = U_r * sqrt_S.unsqueeze(<span class="number">0</span>)  <span class="comment"># [256, r]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(f&quot;  B shape: &#123;B.shape&#125;&quot;)  # 应为 [r, 2560]</span></span><br><span class="line">    <span class="comment"># print(f&quot;  A shape: &#123;A.shape&#125;&quot;)  # 应为 [256, r]</span></span><br><span class="line"></span><br><span class="line">    fc1 = nn.Linear(n, r, bias=<span class="literal">False</span>)</span><br><span class="line">    fc2 = nn.Linear(r, m, bias=linear_layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        fc1.weight.copy_(B)  <span class="comment"># [r, n] = [r, 2560]</span></span><br><span class="line">        fc2.weight.copy_(A)  <span class="comment"># [m, r] = [256, r]</span></span><br><span class="line">        <span class="keyword">if</span> linear_layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            fc2.bias.copy_(linear_layer.bias.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fc1, fc2</span><br></pre></td></tr></table></figure>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p><strong>Linear 层</strong>：<em>W</em>∈R<em>m</em>×<em>n</em>≈<em>U</em>⋅<em>V</em> ，其中 <em>U</em>∈R<em>m</em>×<em>r</em>,<em>V</em>∈R<em>r</em>×<em>n</em> ，<em>r</em>≪min(<em>m</em>,<em>n</em>)</p>
</li>
<li class="lvl-2">
<p><strong>Conv2d 层</strong>：将卷积核张量分解为多个低秩张量（如 CP 分解、Tucker 分解）</p>
</li>
</ul>
</blockquote>
<h4 id="核心参数">核心参数</h4>
<table>
<thead>
<tr>
<th>关键参数</th>
<th>说明</th>
<th>取值范围</th>
<th>改变的影响</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 秩（Rank）<code>r</code></td>
<td>分解后的中间维度</td>
<td>1 \leq r &lt; \min(m, n)</td>
<td>最核心参数！<br>• r ↑ → 精度 ↑，压缩率 ↓<br>• r ↓ → 压缩率 ↑，精度 ↓（可能崩溃）</td>
</tr>
<tr>
<td>2. 秩比例（Rank Ratio）<code>α</code></td>
<td>r = \alpha \cdot \min(m, n)</td>
<td>0.1 \sim 0.9</td>
<td>• α=0.3：高压缩（~70%），高精度损失<br>• α=0.7：中压缩（~30%），低精度损失<br>•  α&gt;0.8：几乎无压缩</td>
</tr>
<tr>
<td>3. 分解目标层</td>
<td>选择哪些层分解</td>
<td>大矩阵优先</td>
<td>• FFN/Linear 层：高收益（参数多）<br>• 小卷积层（3×3）：可能增参<br>• Embedding/LM  Head：通常不分解</td>
</tr>
<tr>
<td>4. 分解方式</td>
<td>SVD / CP / Tucker 等</td>
<td>SVD（主流）</td>
<td>• SVD：理论最优，适合 Linear<br>• CP/Tucker：适合卷积核张量<br>•  NMF：非负约束，适合特定场景</td>
</tr>
<tr>
<td>5. 是否微调（Fine-tuning）</td>
<td>分解后是否训练</td>
<td>是 / 否</td>
<td>• 无微调：精度损失大（尤其 α&lt;0.5）<br>• 有微调：可恢复 80%+ 性能</td>
</tr>
</tbody>
</table>
<h4 id="秩">秩</h4>
<p>改变秩 <code>r</code> 会直接引发一个典型的 <strong>“精度-效率”权衡</strong>。值过低压缩率越好，但是分解后的模型输出大量重复 token</p>
<table>
<thead>
<tr>
<th style="text-align:left">关键指标</th>
<th style="text-align:left"><code>r</code> 增大（更接近原始矩阵）</th>
<th style="text-align:left"><code>r</code> 减小（更激进压缩）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>模型精度</strong></td>
<td style="text-align:left"><strong>↑ 提升</strong> • 重建误差小，更接近原始模型性能。 • 保留更多任务相关特征。</td>
<td style="text-align:left"><strong>↓ 下降</strong> • 重建误差大，信息丢失严重。 • 可能导致模型准确率显著降低。</td>
</tr>
<tr>
<td style="text-align:left"><strong>参数量</strong></td>
<td style="text-align:left"><strong>↑ 增加</strong> • 分解后的矩阵更大。</td>
<td style="text-align:left"><strong>↓ 减少</strong> • 分解后的矩阵更小，压缩率更高。</td>
</tr>
<tr>
<td style="text-align:left"><strong>计算量（FLOPs）</strong></td>
<td style="text-align:left"><strong>↑ 增加</strong> • 需要进行更多次矩阵乘法。</td>
<td style="text-align:left"><strong>↓ 减少</strong> • 计算量显著降低，加速效果更明显。</td>
</tr>
<tr>
<td style="text-align:left"><strong>内存占用</strong></td>
<td style="text-align:left"><strong>↑ 增加</strong> • 需要存储更多的参数。</td>
<td style="text-align:left"><strong>↓ 减少</strong> • 内存占用显著降低。</td>
</tr>
<tr>
<td style="text-align:left"><strong>过拟合风险</strong></td>
<td style="text-align:left"><strong>↑ 增加</strong> • 模型容量相对较大，在小型数据集上可能过拟合。</td>
<td style="text-align:left"><strong>↓ 减少</strong> • 模型容量小，起到正则化作用，可能缓解过拟合。</td>
</tr>
<tr>
<td style="text-align:left"><strong>适用场景</strong></td>
<td style="text-align:left">• 对精度要求高的任务。 <br />• 原始模型冗余度较低。</td>
<td style="text-align:left">• 极度资源受限的边缘设备。 <br />• 对延迟要求极高的场景。 <br />• 原始模型冗余度极高。</td>
</tr>
</tbody>
</table>
<h4 id="秩比例">秩比例</h4>
<table>
<thead>
<tr>
<th>rank_ratio</th>
<th>r (c_fc)</th>
<th>生成质量</th>
<th>参数压缩率</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.3</td>
<td>230</td>
<td>重复</td>
<td>68%</td>
</tr>
<tr>
<td>0.5</td>
<td>384</td>
<td>可用</td>
<td>50%</td>
</tr>
<tr>
<td>0.7</td>
<td>537</td>
<td>接近原始</td>
<td>30%</td>
</tr>
</tbody>
</table>
<p>建议的配置策略：</p>
<table>
<thead>
<tr>
<th>策略</th>
<th>rank_ratio</th>
<th>是否微调</th>
<th>生成质量</th>
<th>压缩率</th>
</tr>
</thead>
<tbody>
<tr>
<td>仅分解 c_proj</td>
<td>0.5</td>
<td>否</td>
<td>良好</td>
<td>25%</td>
</tr>
<tr>
<td>分解全部 + 微调</td>
<td>0.5</td>
<td>是</td>
<td>接近原始</td>
<td>50%</td>
</tr>
<tr>
<td>分解全部（无微调）</td>
<td>0.5</td>
<td>否</td>
<td>轻微重复</td>
<td>50%</td>
</tr>
<tr>
<td>分解全部（无微调）</td>
<td>0.3</td>
<td>否</td>
<td>严重重复</td>
<td>68%</td>
</tr>
</tbody>
</table>
<h4 id="目标层">目标层</h4>
<table>
<thead>
<tr>
<th>层类型</th>
<th>示例</th>
<th>是否推荐分解</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>FFN 中间层</td>
<td>GPT-2 c_fc (768→3072)</td>
<td>谨慎</td>
<td>需高容量编码语义，r 必须大</td>
</tr>
<tr>
<td>FFN 投影层</td>
<td>GPT-2 c_proj (3072→768)</td>
<td>推荐</td>
<td>信息已压缩，可安全分解</td>
</tr>
<tr>
<td>Attention QKV</td>
<td>W_q, W_k, W_v</td>
<td>谨慎</td>
<td>影响注意力质量，需高秩</td>
</tr>
<tr>
<td>分类头</td>
<td>Linear(768→1000)</td>
<td>可分解</td>
<td>任务特定，冗余度高</td>
</tr>
<tr>
<td>Embedding</td>
<td>Token Embedding</td>
<td>不推荐</td>
<td>稀疏激活，分解收益低</td>
</tr>
</tbody>
</table>
<h3 id="二值化-三值化"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Binarization">二值化/三值化</a></h3>
<p>二值化（Binarization）和三值化（Ternarization）是<strong>极致模型压缩技术</strong>，将权重从 FP32 压缩到 <strong>1-bit（±1）或 2-bit（-1, 0, +1）</strong>，适用于<strong>超低功耗边缘设备</strong>（如 MCU、IoT 传感器）。</p>
<blockquote>
<p><strong>二值化/三值化是“空间换精度”的极致压缩</strong>：</p>
<ul class="lvl-1">
<li class="lvl-2"><strong>优势</strong>：模型 &lt;100KB，功耗极低</li>
<li class="lvl-2"><strong>代价</strong>：精度损失，训练复杂</li>
<li class="lvl-2"><strong>适用</strong>：简单任务 + 超低功耗设备</li>
</ul>
</blockquote>
<ol>
<li class="lvl-3">
<p><strong>二值化（Binary Weight Networks, BWN）</strong></p>
<p>权重 <em>W</em>∈R<em>m</em>×<em>n</em> → <em>W**b</em>∈{−1,+1}<em>m</em>×<em>n</em>。前向计算：<em>y</em>=<em>W<strong>b</strong>x</em>≈<em>α</em>⋅<em>W<strong>b</strong>x</em> （<em>α</em> 为缩放因子）</p>
<p><strong>压缩率</strong>：32x（FP32 → 1-bit）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryQuantize</span>(torch.autograd.Function):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    二值化函数（带 STE）</span></span><br><span class="line"><span class="string">    前向: sign(x)</span></span><br><span class="line"><span class="string">    反向: 梯度直通（仅 |x|&lt;=1 时传递）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span></span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># 二值化: &gt;0 → +1, &lt;=0 → -1</span></span><br><span class="line">        out = torch.sign(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># 处理 0（PyTorch sign(0)=0，我们设为 +1）</span></span><br><span class="line">        out[out == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, = ctx.saved_tensors</span><br><span class="line">        <span class="comment"># STE: 梯度仅在 |input| &lt;= 1 时传递</span></span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[torch.<span class="built_in">abs</span>(<span class="built_in">input</span>) &gt; <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BinaryConv2d</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;二值化卷积层&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">        <span class="variable language_">self</span>.out_channels = out_channels</span><br><span class="line">        <span class="variable language_">self</span>.kernel_size = kernel_size</span><br><span class="line">        <span class="variable language_">self</span>.stride = stride</span><br><span class="line">        <span class="variable language_">self</span>.padding = padding</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 浮点权重（用于训练）</span></span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))</span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(out_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 缩放因子 α = mean(|W|)</span></span><br><span class="line">        <span class="variable language_">self</span>.alpha = nn.Parameter(torch.ones(out_channels, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 1. 计算缩放因子 α</span></span><br><span class="line">        alpha = <span class="variable language_">self</span>.weight.<span class="built_in">abs</span>().mean(dim=(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 二值化权重</span></span><br><span class="line">        weight_b = BinaryQuantize.apply(<span class="variable language_">self</span>.weight)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 前向计算: y = α * (x ⊗ W_b) + b</span></span><br><span class="line">        out = F.conv2d(x, weight_b, <span class="literal">None</span>, <span class="variable language_">self</span>.stride, <span class="variable language_">self</span>.padding)</span><br><span class="line">        out = out * alpha + <span class="variable language_">self</span>.bias.view(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
</li>
<li class="lvl-3">
<p><strong>三值化（Ternary Weight Networks, TWN）</strong></p>
<p>权重 <em>W</em>∈R<em>m</em>×<em>n</em> → <em>W**t</em>∈{−1,0,+1}<em>m</em>×<em>n</em>。保留重要权重（非零），移除冗余（零）</p>
<p><strong>压缩率</strong>：16x（FP32 → 2-bit，含稀疏性）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TernaryQuantize</span>(torch.autograd.Function):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    三值化函数（带 STE）</span></span><br><span class="line"><span class="string">    阈值 δ = 0.7 * mean(|W|)</span></span><br><span class="line"><span class="string">    |W| &gt; δ → sign(W), 否则 → 0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, <span class="built_in">input</span>, delta</span>):</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>, delta)</span><br><span class="line">        out = torch.zeros_like(<span class="built_in">input</span>)</span><br><span class="line">        out[<span class="built_in">input</span> &gt; delta] = <span class="number">1</span></span><br><span class="line">        out[<span class="built_in">input</span> &lt; -delta] = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        <span class="built_in">input</span>, delta = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        <span class="comment"># 仅在 [-delta, delta] 外传递梯度</span></span><br><span class="line">        mask = (<span class="built_in">input</span>.<span class="built_in">abs</span>() &gt; delta)</span><br><span class="line">        grad_input[~mask] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TernaryConv2d</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;三值化卷积层&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.in_channels = in_channels</span><br><span class="line">        <span class="variable language_">self</span>.out_channels = out_channels</span><br><span class="line">        <span class="variable language_">self</span>.kernel_size = kernel_size</span><br><span class="line">        <span class="variable language_">self</span>.stride = stride</span><br><span class="line">        <span class="variable language_">self</span>.padding = padding</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))</span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(out_channels))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 计算阈值 δ = 0.7 * mean(|W|)</span></span><br><span class="line">        delta = <span class="number">0.7</span> * <span class="variable language_">self</span>.weight.<span class="built_in">abs</span>().mean()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 三值化权重</span></span><br><span class="line">        weight_t = TernaryQuantize.apply(<span class="variable language_">self</span>.weight, delta)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        out = F.conv2d(x, weight_t, <span class="variable language_">self</span>.bias, <span class="variable language_">self</span>.stride, <span class="variable language_">self</span>.padding)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
</li>
</ol>
<blockquote>
<p>执行结果，模型大小: 0.33 MB (原始 FP32 ~3MB)</p>
</blockquote>
<h3 id="自动轻量化（AutoML）">自动轻量化（AutoML）</h3>
<p>使用神经架构搜索（NAS）或强化学习自动寻找最优轻量结构。</p>
<blockquote>
<ul class="lvl-1">
<li class="lvl-2">
<p>AMC（AutoML for Model Compression）</p>
</li>
<li class="lvl-2">
<p>Once-for-All (OFA)：训练一个超网，可从中提取不同大小的子网。</p>
</li>
</ul>
</blockquote>
<p><strong>实施步骤</strong>：</p>
<ol>
<li class="lvl-3">
<p>定义搜索空间（如通道数、层数、算子类型）。</p>
</li>
<li class="lvl-3">
<p>设计奖励函数（精度 + 延迟约束）。</p>
</li>
<li class="lvl-3">
<p>使用RL、进化算法或梯度方法搜索。</p>
</li>
<li class="lvl-3">
<p>训练最优子网并部署。</p>
</li>
</ol>
<h2 id="端到端实施流程">端到端实施流程</h2>
<ol>
<li class="lvl-3">
<p>基准建立</p>
<ul class="lvl-2">
<li class="lvl-5">在目标任务上训练一个高精度模型（如ResNet50）。</li>
<li class="lvl-5">测量其参数量、FLOPs、推理延迟、内存占用。</li>
</ul>
</li>
<li class="lvl-3">
<p><strong>选择轻量化策略组合</strong></p>
<ul class="lvl-2">
<li class="lvl-5">优先考虑结构设计 + 量化（通用性强）。</li>
<li class="lvl-5">若精度要求高，加入知识蒸馏。</li>
<li class="lvl-5">若已有大模型，可尝试剪枝 + QAT。</li>
</ul>
</li>
<li class="lvl-3">
<p><strong>迭代优化</strong></p>
<img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="/imgs/ai/image-20251012153208075.png" alt="image-20251012153208075" style="zoom:67%;" />
</li>
<li class="lvl-3">
<p><strong>部署验证</strong></p>
<ul class="lvl-2">
<li class="lvl-5">使用目标硬件（如ARM CPU、NPU）测试实际延迟与功耗。</li>
<li class="lvl-5">使用TFLite、ONNX、TensorRT等格式转换工具。</li>
</ul>
</li>
</ol>
<h2 id="GPT2-轻量化实现">GPT2 轻量化实现</h2>
<p>通过以下技术对 gpt2 模型的轻量化 demo 实现。</p>
<p>微调恢复精度:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fine_tune_pruned</span>(<span class="params">model, tokenizer, num_steps=<span class="number">100</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;微调剪枝后模型（恢复精度）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; 开始微调剪枝后模型...&quot;</span>)</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 简单文本用于微调</span></span><br><span class="line">    texts = [</span><br><span class="line">                <span class="string">&quot;Artificial intelligence is a wonderful field.&quot;</span>,</span><br><span class="line">                <span class="string">&quot;Machine learning enables computers to learn from data.&quot;</span>,</span><br><span class="line">                <span class="string">&quot;Natural language processing allows machines to understand human language.&quot;</span>,</span><br><span class="line">                <span class="string">&quot;The future of AI is bright and full of possibilities.&quot;</span>,</span><br><span class="line">                <span class="string">&quot;Deep learning has revolutionized many areas of technology.&quot;</span></span><br><span class="line">            ] * <span class="number">20</span>  <span class="comment"># 重复以增加数据量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        text = texts[step % <span class="built_in">len</span>(texts)]</span><br><span class="line">        inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>, max_length=<span class="number">128</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">        input_ids = inputs.input_ids.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = model(input_ids, labels=input_ids)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;  Step <span class="subst">&#123;step&#125;</span>, Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h3 id="模型剪枝"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Pruning/gpt2">模型剪枝</a></h3>
<p>剪枝</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prune_gpt2_mlp</span>(<span class="params">model: GPT2LMHeadModel, prune_ratio=<span class="number">0.3</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对 GPT-2 的 MLP 层进行结构化剪枝</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    剪枝策略:</span></span><br><span class="line"><span class="string">    - 评估 c_fc 和 c_proj 的输出通道重要性</span></span><br><span class="line"><span class="string">    - 移除 L1 范数最小的 prune_ratio 比例通道</span></span><br><span class="line"><span class="string">    - 重建模型结构确保维度匹配</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot; 开始剪枝 GPT-2 MLP 层 (prune_ratio=<span class="subst">&#123;prune_ratio&#125;</span>)...&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer_idx, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.transformer.h):</span><br><span class="line">        <span class="comment"># print(f&quot;  处理层 &#123;layer_idx&#125;...&quot;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># === 1. 剪枝 c_fc (768 -&gt; 3072) ===</span></span><br><span class="line">        c_fc = block.mlp.c_fc</span><br><span class="line">        l1_norm_fc = get_conv1d_l1_norm(c_fc)</span><br><span class="line">        num_total_fc = <span class="built_in">len</span>(l1_norm_fc)</span><br><span class="line">        num_keep_fc = <span class="built_in">int</span>(num_total_fc * (<span class="number">1</span> - prune_ratio))</span><br><span class="line">        _, keep_indices_fc = torch.topk(l1_norm_fc, num_keep_fc, largest=<span class="literal">True</span>)</span><br><span class="line">        pruned_c_fc = prune_conv1d_layer(c_fc, keep_indices_fc)</span><br><span class="line">        block.mlp.c_fc = pruned_c_fc</span><br><span class="line"></span><br><span class="line">        <span class="comment"># === 2. 剪枝 c_proj 的 INPUT 通道（3072 -&gt; num_keep_fc）===</span></span><br><span class="line">        c_proj = block.mlp.c_proj</span><br><span class="line">        <span class="comment"># c_proj.weight.shape = [3072, 768] → 输入通道是第 0 维</span></span><br><span class="line">        weight_proj = c_proj.weight.data  <span class="comment"># [in=3072, out=768]</span></span><br><span class="line">        bias_proj = c_proj.bias.data <span class="keyword">if</span> c_proj.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 剪枝输入通道：保留 keep_indices_fc 对应的行</span></span><br><span class="line">        pruned_weight_proj = weight_proj[keep_indices_fc, :]  <span class="comment"># [num_keep_fc, 768]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建新 c_proj: in=num_keep_fc, out=768</span></span><br><span class="line">        new_c_proj = Conv1D(<span class="number">768</span>, num_keep_fc)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            new_c_proj.weight.copy_(pruned_weight_proj)</span><br><span class="line">            <span class="keyword">if</span> bias_proj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                new_c_proj.bias.copy_(bias_proj)  <span class="comment"># bias 不变（输出维度仍是 768）</span></span><br><span class="line"></span><br><span class="line">        block.mlp.c_proj = new_c_proj</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot; GPT-2 MLP 剪枝完成！&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>索引剪枝层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">prune_conv1d_layer</span>(<span class="params">conv1d_layer: Conv1D, keep_indices: torch.Tensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据保留的输出通道索引剪枝 Conv1D 层</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        conv1d_layer: 原始 Conv1D 层</span></span><br><span class="line"><span class="string">        keep_indices: 要保留的输出通道索引 (shape=[num_keep])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        新的 Conv1D 层（输出通道数 = num_keep）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    weight = conv1d_layer.weight.data  <span class="comment"># [in, out]</span></span><br><span class="line">    bias = conv1d_layer.bias.data <span class="keyword">if</span> conv1d_layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 剪枝权重: 保留 keep_indices 对应的输出通道</span></span><br><span class="line">    pruned_weight = weight[:, keep_indices]  <span class="comment"># [in, num_keep]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建新层</span></span><br><span class="line">    in_features = weight.shape[<span class="number">0</span>]</span><br><span class="line">    out_features = <span class="built_in">len</span>(keep_indices)</span><br><span class="line">    new_layer = Conv1D(out_features, in_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        new_layer.weight.copy_(pruned_weight)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            new_layer.bias.copy_(bias[keep_indices])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_layer</span><br></pre></td></tr></table></figure>
<h3 id="量化感知"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Quantization/gpt2">量化感知</a></h3>
<p>待完善</p>
<h3 id="知识蒸馏"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Distillation/gpt2">知识蒸馏</a></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Teacher: gpt2-medium (355M 参数)</span></span><br><span class="line">teacher = GPT2LMHeadModel.from_pretrained(<span class="string">&quot;openai-community/gpt2-medium&quot;</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Student: gpt2 (124M 参数)</span></span><br><span class="line">student = GPT2LMHeadModel.from_pretrained(<span class="string">&quot;openai-community/gpt2&quot;</span>).to(device)</span><br></pre></td></tr></table></figure>
<p>蒸馏损失：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_language_modeling_loss</span>(<span class="params">logits, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;标准语言建模损失&quot;&quot;&quot;</span></span><br><span class="line">    shift_logits = logits[..., :-<span class="number">1</span>, :].contiguous()</span><br><span class="line">    shift_labels = labels[..., <span class="number">1</span>:].contiguous()</span><br><span class="line">    loss = fun.cross_entropy(</span><br><span class="line">        shift_logits.view(-<span class="number">1</span>, shift_logits.size(-<span class="number">1</span>)),</span><br><span class="line">        shift_labels.view(-<span class="number">1</span>),</span><br><span class="line">        ignore_index=-<span class="number">100</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">distillation_loss</span>(<span class="params">student_logits, teacher_logits, labels,</span></span><br><span class="line"><span class="params">                      alpha=<span class="number">0.5</span>, temperature=<span class="number">1.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    知识蒸馏损失 = α * 软标签损失 + (1-α) * 硬标签损失</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        student_logits: Student 模型输出 [B, T, V]</span></span><br><span class="line"><span class="string">        teacher_logits: Teacher 模型输出 [B, T, V]</span></span><br><span class="line"><span class="string">        labels: 真实标签 [B, T]</span></span><br><span class="line"><span class="string">        alpha: 软标签损失权重（0.5~0.9）</span></span><br><span class="line"><span class="string">        temperature: 温度参数（&gt;1 平滑分布）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># === 1. 软标签损失（KL 散度）===</span></span><br><span class="line">    <span class="comment"># Teacher 软化概率分布</span></span><br><span class="line">    teacher_probs = fun.log_softmax(teacher_logits / temperature, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># Student 软化概率分布</span></span><br><span class="line">    student_probs = fun.log_softmax(student_logits / temperature, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># KL 散度损失</span></span><br><span class="line">    kl_loss = fun.kl_div(</span><br><span class="line">        student_probs,</span><br><span class="line">        teacher_probs,</span><br><span class="line">        reduction=<span class="string">&#x27;batchmean&#x27;</span></span><br><span class="line">    ) * (temperature ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 硬标签损失（高权重）</span></span><br><span class="line">    ce_loss = compute_language_modeling_loss(student_logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 总损失：KL 权重更低</span></span><br><span class="line">    total_loss = alpha * kl_loss + (<span class="number">1</span> - alpha) * ce_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss, kl_loss, ce_loss</span><br></pre></td></tr></table></figure>
<p>使用以上蒸馏技术后效果：</p>
<blockquote>
<p>Student (蒸馏后)：The future of AI is,（出现 Token 循环问题）</p>
<p>困惑度对比（500 样本）:</p>
<ul class="lvl-1">
<li class="lvl-4">
<p>Teacher (gpt2-medium): 40.44</p>
</li>
<li class="lvl-4">
<p>Student (原始 gpt2):   54.64</p>
</li>
<li class="lvl-4">
<p>Student (蒸馏后):      123638626614094164131840.00</p>
</li>
</ul>
</blockquote>
<p>通过调整参数（alpha、temperature、total_loss）后，效果甚微。</p>
<p>改用三阶段蒸馏后，效果如下：</p>
<blockquote>
<p>Student (蒸馏后): The future of AI is still a mystery.</p>
<p>困惑度对比（200 样本）:</p>
<ul class="lvl-1">
<li class="lvl-4">
<p>Teacher (gpt2-medium): 35.91</p>
</li>
<li class="lvl-4">
<p>Student (原始 gpt2):   49.94</p>
</li>
<li class="lvl-4">
<p>Student (蒸馏后):      36.26</p>
</li>
</ul>
</blockquote>
<h3 id="低秩分解"><a target="_blank" rel="noopener" href="https://github.com/HengLine/ai-model-lightweighting/tree/main/Low-Rank/gpt2">低秩分解</a></h3>
<p>低秩分解的核心方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decompose_conv1d_svd</span>(<span class="params">conv1d_layer: Conv1D, rank_ratio=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用奇异值分解（SVD）对 Hugging Face 的 Conv1D 层进行低秩近似分解。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    背景说明：</span></span><br><span class="line"><span class="string">    - Hugging Face 的 GPT-2/Transformer 模型使用自定义 Conv1D 层（非标准 nn.Conv1d）</span></span><br><span class="line"><span class="string">    - Conv1D(out_features, in_features) 表示：输入 in_features 维，输出 out_features 维</span></span><br><span class="line"><span class="string">    - 其权重矩阵 W 的形状为 [in_features, out_features]（注意：与 nn.Linear 相反！）</span></span><br><span class="line"><span class="string">    - 前向计算公式：output = input @ W + bias （标准矩阵乘法，无需转置）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    分解目标：</span></span><br><span class="line"><span class="string">    将原始权重矩阵 W ∈ R^&#123;n×m&#125;（n=in_features, m=out_features）近似分解为：</span></span><br><span class="line"><span class="string">        W ≈ A @ B</span></span><br><span class="line"><span class="string">    其中：</span></span><br><span class="line"><span class="string">        A ∈ R^&#123;n×r&#125;  （第一层权重）</span></span><br><span class="line"><span class="string">        B ∈ R^&#123;r×m&#125;  （第二层权重）</span></span><br><span class="line"><span class="string">        r = rank_ratio * min(n, m)  （低秩近似秩）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    优势：</span></span><br><span class="line"><span class="string">    - 原始参数量：n × m</span></span><br><span class="line"><span class="string">    - 分解后参数量：n × r + r × m = r × (n + m)</span></span><br><span class="line"><span class="string">    - 当 r &lt;&lt; min(n, m) 时，显著减少参数量和计算量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        conv1d_layer (Conv1D): 待分解的 Hugging Face Conv1D 层</span></span><br><span class="line"><span class="string">        rank_ratio (float): 保留的秩比例（0.0 ~ 1.0），值越小压缩率越高，但精度损失越大</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        nn.Sequential: 由两个 Conv1D 层组成的序列，功能等价于原始层（近似）</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 1: 提取原始权重和偏置 ===</span></span><br><span class="line">    <span class="comment"># Conv1D.weight 形状: [in_features, out_features]</span></span><br><span class="line">    W = conv1d_layer.weight.data  <span class="comment"># 获取权重张量（不计算梯度）</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取偏置（如果存在）</span></span><br><span class="line">    <span class="comment"># Conv1D.bias 形状: [out_features]</span></span><br><span class="line">    bias = conv1d_layer.bias.data <span class="keyword">if</span> conv1d_layer.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 2: 获取输入/输出维度 ===</span></span><br><span class="line">    <span class="comment"># W.shape = [in_features, out_features]</span></span><br><span class="line">    in_features, out_features = W.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算最大可能秩（矩阵的秩不超过 min(行数, 列数)）</span></span><br><span class="line">    max_rank = <span class="built_in">min</span>(in_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据 rank_ratio 计算目标分解秩 r</span></span><br><span class="line">    r = <span class="built_in">int</span>(rank_ratio * max_rank)</span><br><span class="line">    <span class="comment"># 确保 r 至少为 1（避免秩为 0）</span></span><br><span class="line">    r = <span class="built_in">max</span>(<span class="number">1</span>, r)</span><br><span class="line">    <span class="comment"># 确保 r 不超过 max_rank - 1（避免数值不稳定，且 SVD 需要 r &lt; 秩）</span></span><br><span class="line">    r = <span class="built_in">min</span>(r, max_rank - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 3: 执行奇异值分解（SVD）===</span></span><br><span class="line">    <span class="comment"># 对权重矩阵 W 执行 SVD: W = U @ diag(S) @ Vh</span></span><br><span class="line">    <span class="comment"># - U: 左奇异向量矩阵，形状 [in_features, max_rank]</span></span><br><span class="line">    <span class="comment"># - S: 奇异值向量，形状 [max_rank]</span></span><br><span class="line">    <span class="comment"># - Vh: 右奇异向量矩阵的转置，形状 [max_rank, out_features]</span></span><br><span class="line">    <span class="comment"># 使用 full_matrices=False 以节省内存（只计算必要部分）</span></span><br><span class="line">    U, S, Vh = torch.linalg.svd(W, full_matrices=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 4: 提取前 r 个奇异分量 ===</span></span><br><span class="line">    <span class="comment"># 取前 r 列的左奇异向量: [in_features, r]</span></span><br><span class="line">    U_r = U[:, :r]</span><br><span class="line">    <span class="comment"># 取前 r 个奇异值: [r]</span></span><br><span class="line">    S_r = S[:r]</span><br><span class="line">    <span class="comment"># 取前 r 行的右奇异向量转置: [r, out_features]</span></span><br><span class="line">    Vh_r = Vh[:r, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 5: 构造低秩近似矩阵 ===</span></span><br><span class="line">    <span class="comment"># 数学原理：W ≈ U_r @ diag(S_r) @ Vh_r</span></span><br><span class="line">    <span class="comment"># 为数值稳定性和对称性，将奇异值平方根分配到两边：</span></span><br><span class="line">    <span class="comment">#   A = U_r @ diag(sqrt(S_r))  → [in_features, r]</span></span><br><span class="line">    <span class="comment">#   B = diag(sqrt(S_r)) @ Vh_r  → [r, out_features]</span></span><br><span class="line">    <span class="comment"># 这样 A @ B = U_r @ diag(S_r) @ Vh_r ≈ W</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 sqrt(S_r) 并扩展维度以支持广播</span></span><br><span class="line">    sqrt_S = torch.sqrt(S_r)  <span class="comment"># [r]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造 A = U_r * sqrt(S_r)</span></span><br><span class="line">    <span class="comment"># 使用 unsqueeze(0) 将 sqrt_S 变为 [1, r]，与 U_r [in_features, r] 广播相乘</span></span><br><span class="line">    A = U_r * sqrt_S.unsqueeze(<span class="number">0</span>)  <span class="comment"># 结果形状: [in_features, r]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造 B = sqrt(S_r) * Vh_r</span></span><br><span class="line">    <span class="comment"># 使用 unsqueeze(1) 将 sqrt_S 变为 [r, 1]，与 Vh_r [r, out_features] 广播相乘</span></span><br><span class="line">    B = sqrt_S.unsqueeze(<span class="number">1</span>) * Vh_r  <span class="comment"># 结果形状: [r, out_features]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 6: 创建两个新的 Conv1D 层 ===</span></span><br><span class="line">    <span class="comment"># 第一层: 输入 in_features → 输出 r</span></span><br><span class="line">    <span class="comment"># Conv1D(out_features=r, in_features=in_features)</span></span><br><span class="line">    conv1 = Conv1D(r, in_features)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二层: 输入 r → 输出 out_features</span></span><br><span class="line">    <span class="comment"># Conv1D(out_features=out_features, in_features=r)</span></span><br><span class="line">    <span class="comment"># 注意：偏置只加在最后一层（与原始层一致）</span></span><br><span class="line">    conv2 = Conv1D(out_features, r)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 7: 复制分解后的权重和偏置 ===</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算，避免影响优化器状态</span></span><br><span class="line">        <span class="comment"># 复制第一层权重 A ([in_features, r])</span></span><br><span class="line">        <span class="comment"># conv1.weight 形状应为 [in_features, r]</span></span><br><span class="line">        conv1.weight.copy_(A)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 复制第二层权重 B ([r, out_features])</span></span><br><span class="line">        <span class="comment"># conv2.weight 形状应为 [r, out_features]</span></span><br><span class="line">        conv2.weight.copy_(B)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 复制偏置（仅第二层需要，因为原始偏置作用于最终输出）</span></span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># conv2.bias 形状: [out_features]</span></span><br><span class="line">            conv2.bias.copy_(bias)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># === 步骤 8: 返回组合层 ===</span></span><br><span class="line">    <span class="comment"># 使用 nn.Sequential 将两个 Conv1D 层串联</span></span><br><span class="line">    <span class="comment"># 前向计算: input -&gt; conv1 -&gt; conv2 -&gt; output</span></span><br><span class="line">    <span class="comment"># 功能等价于: output = input @ W + bias （近似）</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(conv1, conv2)</span><br></pre></td></tr></table></figure>
<p>分解 + 生成对比</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    model_name = <span class="string">&quot;openai-community/gpt2&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 加载原始 GPT-2</span></span><br><span class="line">    tokenizer = GPT2Tokenizer.from_pretrained(model_name)</span><br><span class="line">    model_orig = GPT2LMHeadModel.from_pretrained(model_name).to(device)</span><br><span class="line">    model_orig.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;原始模型参数量: <span class="subst">&#123;count_parameters(model_orig):<span class="number">.2</span>f&#125;</span> M&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 复制模型用于分解</span></span><br><span class="line">    model_decomp = GPT2LMHeadModel.from_pretrained(model_name).to(device)</span><br><span class="line">    model_decomp.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 低秩分解 0.3 重复; 0.5 可用; 0.6 较好; 0.7 接近原始</span></span><br><span class="line">    <span class="comment"># 分解全部 + 微调</span></span><br><span class="line">    <span class="comment"># model_decomp = decompose_gpt2_mlp(model_decomp, rank_ratio=0.5)</span></span><br><span class="line">    <span class="comment"># model_decomp = fine_tune_decomposed(model_decomp, tokenizer, num_steps=60)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 推荐：只分解 c_proj + rank_ratio=0.6</span></span><br><span class="line">    model_decomp = decompose_gpt2_mlp(model_decomp, rank_ratio=<span class="number">0.6</span>, decompose_c_fc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;分解后模型参数量: <span class="subst">&#123;count_parameters(model_decomp):<span class="number">.2</span>f&#125;</span> M&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 文本生成对比</span></span><br><span class="line">    prompt = <span class="string">&quot;The future of AI is&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n 提示词: &#x27;<span class="subst">&#123;prompt&#125;</span>&#x27;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    text_orig = generate_text(model_orig, tokenizer, prompt)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n 原始模型:\n<span class="subst">&#123;text_orig&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    text_decomp = generate_text(model_decomp, tokenizer, prompt)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n 分解模型:\n<span class="subst">&#123;text_decomp&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 保存分解后模型</span></span><br><span class="line">    model_decomp.save_pretrained(<span class="string">&quot;../../data/gpt2_decomposed&quot;</span>)</span><br><span class="line">    tokenizer.save_pretrained(<span class="string">&quot;../../data/gpt2_decomposed&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="不同场景的轻量化">不同场景的轻量化</h2>
<p><strong>同一模型在不同场景下的轻量化策略应有所不同</strong>。轻量化不是“一刀切”的技术，而是需要根据<strong>部署硬件、应用场景、性能约束和业务需求</strong>进行定制化设计。</p>
<blockquote>
<p>轻量化的本质是在 <strong>精度（Accuracy）</strong>、<strong>速度（Latency）</strong>、<strong>体积（Size）</strong> 和 <strong>功耗（Power）</strong> 之间做权衡。不同场景对这些指标的优先级完全不同。</p>
</blockquote>
<table>
<thead>
<tr>
<th>场景</th>
<th>硬件平台</th>
<th>关键约束</th>
<th>推荐轻量化技术</th>
<th>原因</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 手机端实时语音助手</td>
<td>ARM CPU / NPU</td>
<td>低延迟（&lt;300ms）<br>中等精度</td>
<td>1.量化（INT8）<br>2.结构化剪枝<br>3.轻量架构（Squeezeformer）</td>
<td>- CPU/NPU 对 INT8 有硬件加速<br>- 延迟敏感，需移除冗余计算<br>- 精度可小幅牺牲（WER  +0.5% 可接受）</td>
</tr>
<tr>
<td>2. 智能音箱（离线唤醒）</td>
<td>MCU / DSP（&lt;100MHz）</td>
<td>极低功耗<br>模型 &lt;1MB<br>低精度</td>
<td>1.二值化/三值化<br>2.知识蒸馏（Tiny Student）<br>3.算子融合 + 固定点</td>
<td>- 内存极小，需极致压缩<br/>- 唤醒词任务简单，小模型足够<br/>- 浮点运算耗电，需定点</td>
</tr>
<tr>
<td>3. 车载语音系统</td>
<td>Automotive SoC（如 Qualcomm SA8155）</td>
<td>高可靠性<br>实时性（&lt;500ms）<br>中高精度</td>
<td>1.量化感知训练（QAT）<br>2.通道剪枝 + 微调<br>3.模型分割（CPU+NPU）</td>
<td>- 安全关键，精度损失需 &lt;0.3%<br/>- SoC 有专用 NPU，需 INT8 优化<br/>- 需支持多语言，模型不能太小</td>
</tr>
<tr>
<td>4. 服务器高并发 API</td>
<td>x86 CPU / GPU</td>
<td>高吞吐（QPS）<br>低延迟</td>
<td>1.TensorRT INT8（GPU）<br>2.ONNX + 并行推理<br>3.动态批处理</td>
<td>- GPU 用 TensorRT 效果远超 PTQ<br/>- CPU 用 OpenVINO / ONNX Runtime<br/>- 吞吐优先，可接受稍大模型</td>
</tr>
<tr>
<td>5. IoT 传感器（关键词检测）</td>
<td>Cortex-M 系列</td>
<td>&lt;100KB 模型<br>&lt;10mW 功耗</td>
<td>1.MCU 专用框架（TensorFlow）<br>2.手工设计 Tiny CNN<br>3.无浮点，全整型</td>
<td>- 无操作系统，需静态内存<br>- 模型必须 &lt;100KB<br>- 通常只检测 1~10 个关键词</td>
</tr>
</tbody>
</table>
<h3 id="场景-1：手机-App（Android）">场景 1：手机 App（Android）</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>目标</strong>：实时转录，延迟 &lt;500ms</p>
</li>
<li class="lvl-2">
<p>策略：</p>
<ul class="lvl-2">
<li class="lvl-4">使用 <strong>WeNet + Squeezeformer</strong></li>
<li class="lvl-4"><strong>INT8 量化</strong>（TFLite + NNAPI）</li>
<li class="lvl-4"><strong>剪枝 30% 通道</strong></li>
<li class="lvl-4">模型大小：<strong>~5MB</strong></li>
<li class="lvl-4">WER：<strong>+0.8%</strong></li>
</ul>
</li>
</ul>
<h3 id="场景-2：智能手表（Wear-OS）">场景 2：智能手表（Wear OS）</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>目标</strong>：离线命令识别，模型 &lt;2MB</p>
</li>
<li class="lvl-2">
<p>策略：</p>
<ul class="lvl-2">
<li class="lvl-4"><strong>知识蒸馏</strong>：Teacher=Conformer, Student=Tiny-Conformer（4 层）</li>
<li class="lvl-4"><strong>FP16 量化</strong>（无 INT8 支持）</li>
<li class="lvl-4">移除语言模型（仅 CTC）</li>
<li class="lvl-4">模型大小：<strong>~1.8MB</strong></li>
<li class="lvl-4">WER：<strong>+2.5%</strong>（可接受，因命令简单）</li>
</ul>
</li>
</ul>
<h3 id="场景-3：车载系统（Linux-NPU）">场景 3：车载系统（Linux + NPU）</h3>
<ul class="lvl-0">
<li class="lvl-2">
<p><strong>目标</strong>：多语言支持，高鲁棒性</p>
</li>
<li class="lvl-2">
<p>策略：</p>
<ul class="lvl-2">
<li class="lvl-4"><strong>QAT（量化感知训练）</strong></li>
<li class="lvl-4"><strong>仅剪枝 10%</strong>（保留精度）</li>
<li class="lvl-4"><strong>TensorRT 导出</strong></li>
<li class="lvl-4">模型大小：<strong>~8MB</strong></li>
<li class="lvl-4">WER：<strong>+0.2%</strong></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>技术</th>
<th>适用场景</th>
<th>不适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>量化（PTQ/QAT）</td>
<td>手机、服务器、车载（有 INT8 支持）</td>
<td>MCU（无 SIMD 指令）</td>
</tr>
<tr>
<td>结构化剪枝</td>
<td>通用（CPU/GPU/NPU）</td>
<td>超小模型（剪枝收益低）</td>
</tr>
<tr>
<td>知识蒸馏</td>
<td>需要小模型 + 有大 Teacher</td>
<td>无预训练大模型</td>
</tr>
<tr>
<td>低秩分解</td>
<td>大 Linear 层（如 FFN）</td>
<td>小卷积层（如 3x3）</td>
</tr>
<tr>
<td>轻量架构设计</td>
<td>从零训练</td>
<td>已有大模型需压缩</td>
</tr>
<tr>
<td>二值化/三值化</td>
<td>MCU、超低功耗</td>
<td>高精度任务（如医疗 ASR）</td>
</tr>
</tbody>
</table>
<h2 id="常用工具与框架">常用工具与框架</h2>
<table>
<thead>
<tr>
<th>技术</th>
<th>工具/库</th>
</tr>
</thead>
<tbody>
<tr>
<td>剪枝</td>
<td>NNI, TorchPruner, TensorFlow Model Optimization</td>
</tr>
<tr>
<td>量化</td>
<td>PyTorch Quantization, TensorFlow Lite, TensorRT</td>
</tr>
<tr>
<td>蒸馏</td>
<td>HuggingFace Transformers (DistilBERT), TorchDistill</td>
</tr>
<tr>
<td>轻量网络</td>
<td>timm (PyTorch Image Models), torchvision</td>
</tr>
<tr>
<td>自动压缩</td>
<td>AutoKeras, OFA official repo</td>
</tr>
</tbody>
</table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://pengline.github.io">余一叶知秋尽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://pengline.github.io/2025/10/1332727d5b8a4fdc9cfbfd3661f009d4/">https://pengline.github.io/2025/10/1332727d5b8a4fdc9cfbfd3661f009d4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://pengline.github.io" target="_blank">余一叶知秋尽</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI%E6%A8%A1%E5%9E%8B%E8%BD%BB%E9%87%8F%E5%8C%96/">AI模型轻量化</a><a class="post-meta__tags" href="/tags/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D/">模型剪枝</a><a class="post-meta__tags" href="/tags/%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5/">量化感知</a><a class="post-meta__tags" href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/">知识蒸馏</a><a class="post-meta__tags" href="/tags/%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3/">低秩分解</a></div><div class="post-share"><div class="social-share" data-image="https://www.toopic.cn/public/uploads/small/1755144020384175514402028.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/10/e957152912ce463aa001ac0dda36a695/" title="系统架构技术选型之 MQ 篇"><img class="cover" src= "/img/20241215205335173426721579192.jpg" data-lazy-src="/img/essay/small231205PbwhI1753715525.jpg" onerror="onerror=null;src='/img/k3zssqvjw2d.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">系统架构技术选型之 MQ 篇</div></div><div class="info-2"><div class="info-item-1">主流 MQ 包括 Kafka、RabbitMQ、RocketMQ。本文从多个维度对这三者进行详细对比分析，帮助进行技术选型。</div></div></div></a></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E6%89%8B%E6%AE%B5%E4%B8%8E%E8%B7%AF%E7%BA%BF"><span class="toc-number">1.</span> <span class="toc-text">技术手段与路线</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.1.</span> <span class="toc-text">网络结构设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D%EF%BC%88Pruning%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">模型剪枝（Pruning）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AA%E6%9E%9D%E6%8A%80%E6%9C%AF%E5%92%8C%E7%B1%BB%E5%88%AB"><span class="toc-number">1.2.1.</span> <span class="toc-text">剪枝技术和类别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%AE%97%E6%B3%95"><span class="toc-number">1.2.2.</span> <span class="toc-text">核心算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%96%BD%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.2.3.</span> <span class="toc-text">实施步骤</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%EF%BC%88Quantization%EF%BC%89"><span class="toc-number">1.3.</span> <span class="toc-text">量化感知（Quantization）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GPTQ-%E9%87%8F%E5%8C%96"><span class="toc-number">1.3.1.</span> <span class="toc-text">GPTQ 量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PyTorch-%E9%87%8F%E5%8C%96"><span class="toc-number">1.3.2.</span> <span class="toc-text">PyTorch 量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TensorRT-%E9%9B%86%E6%88%90"><span class="toc-number">1.3.3.</span> <span class="toc-text">TensorRT 集成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E6%96%BD"><span class="toc-number">1.3.4.</span> <span class="toc-text">具体实施</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%EF%BC%88Distillation%EF%BC%89"><span class="toc-number">1.4.</span> <span class="toc-text">知识蒸馏（Distillation）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86"><span class="toc-number">1.4.1.</span> <span class="toc-text">核心原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="toc-number">1.4.2.</span> <span class="toc-text">训练策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%96%BD%E6%AD%A5%E9%AA%A4-2"><span class="toc-number">1.4.3.</span> <span class="toc-text">实施步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Token-%E5%BE%AA%E7%8E%AF%E9%97%AE%E9%A2%98"><span class="toc-number">1.4.4.</span> <span class="toc-text">Token 循环问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3%EF%BC%88Low-Rank%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">低秩分解（Low-Rank）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8F%82%E6%95%B0"><span class="toc-number">1.5.1.</span> <span class="toc-text">核心参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A7%A9"><span class="toc-number">1.5.2.</span> <span class="toc-text">秩</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A7%A9%E6%AF%94%E4%BE%8B"><span class="toc-number">1.5.3.</span> <span class="toc-text">秩比例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%B1%82"><span class="toc-number">1.5.4.</span> <span class="toc-text">目标层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%80%BC%E5%8C%96-%E4%B8%89%E5%80%BC%E5%8C%96"><span class="toc-number">1.6.</span> <span class="toc-text">二值化&#x2F;三值化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E8%BD%BB%E9%87%8F%E5%8C%96%EF%BC%88AutoML%EF%BC%89"><span class="toc-number">1.7.</span> <span class="toc-text">自动轻量化（AutoML）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E5%AE%9E%E6%96%BD%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">端到端实施流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT2-%E8%BD%BB%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">GPT2 轻量化实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D"><span class="toc-number">3.1.</span> <span class="toc-text">模型剪枝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5"><span class="toc-number">3.2.</span> <span class="toc-text">量化感知</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-number">3.3.</span> <span class="toc-text">知识蒸馏</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3"><span class="toc-number">3.4.</span> <span class="toc-text">低秩分解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E5%9C%BA%E6%99%AF%E7%9A%84%E8%BD%BB%E9%87%8F%E5%8C%96"><span class="toc-number">4.</span> <span class="toc-text">不同场景的轻量化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF-1%EF%BC%9A%E6%89%8B%E6%9C%BA-App%EF%BC%88Android%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">场景 1：手机 App（Android）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF-2%EF%BC%9A%E6%99%BA%E8%83%BD%E6%89%8B%E8%A1%A8%EF%BC%88Wear-OS%EF%BC%89"><span class="toc-number">4.2.</span> <span class="toc-text">场景 2：智能手表（Wear OS）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF-3%EF%BC%9A%E8%BD%A6%E8%BD%BD%E7%B3%BB%E7%BB%9F%EF%BC%88Linux-NPU%EF%BC%89"><span class="toc-number">4.3.</span> <span class="toc-text">场景 3：车载系统（Linux + NPU）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E4%B8%8E%E6%A1%86%E6%9E%B6"><span class="toc-number">5.</span> <span class="toc-text">常用工具与框架</span></a></li></ol></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By 余一叶知秋尽</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="https://img.shields.io/badge/Hosted-Github-brightgreen?style=flat&logo=GitHub"title="本站项目由Gtihub托管"></a><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?logoColor=white&style=flat&logo=buefy"title="主题采用butterfly"></a><a style="margin-inline:5px"target="_blank"href="https://giscus.app/zh-CN/"><img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="https://img.shields.io/badge/Comment-Giscus-2873df?logoColor=white&style=flat&logo=git"title="评论系统为Giscus"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src= "/img/20241215205335173426721579192.jpg" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="日间和夜间模式切换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'pengline/pengline.github.io',
      'data-repo-id': 'R_kgDOPqG50w',
      'data-category-id': 'DIC_kwDOPqG5084Cu_K9',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      'data-lang': 'zh-CN',
      'data-input-position': 'top',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme),
            lang: 'zh-CN'
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script src="/js/sun_moon.js" async></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>